{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.\\nSuppose we have a simple prompt + model sequence:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nprompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Write out the following equation using algebraic symbols then solve it. Use the format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\", ), (\"human\", \"{equation_statement}\"), ] ) model = ChatOpenAI(temperature=0) runnable = ( {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser() )\\nprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))\\nEQUATION: x^3 + 7 = 12\\nSOLUTION: Subtracting 7 from both sides of the equation, we get: x^3 = 12 - 7 x^3 = 5\\nTaking the cube root of both sides, we get: x = ∛5\\nTherefore, the solution to the equation x^3 + 7 = 12 is x = ∛5.\\nand want to call the model with certain stop words:\\nrunnable = ( {\"equation_statement\": RunnablePassthrough()} | prompt | model.bind(stop=\"SOLUTION\") | StrOutputParser() ) print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\\nEQUATION: x^3 + 7 = 12\\nAttaching OpenAI functions One particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:\\nfunction = { \"name\": \"solver\", \"description\": \"Formulates and solves an equation\", \"parameters\": { \"type\": \"object\", \"properties\": { \"equation\": { \"type\": \"string\", \"description\": \"The algebraic expression of the equation\", }, \"solution\": { \"type\": \"string\", \"description\": \"The solution to the equation\", }, }, \"required\": [\"equation\", \"solution\"], }, }', metadata={'source': './files/1_2_1_Bind_runtime_args.txt'}),\n",
       " Document(page_content='function = { \"name\": \"solver\", \"description\": \"Formulates and solves an equation\", \"parameters\": { \"type\": \"object\", \"properties\": { \"equation\": { \"type\": \"string\", \"description\": \"The algebraic expression of the equation\", }, \"solution\": { \"type\": \"string\", \"description\": \"The solution to the equation\", }, }, \"required\": [\"equation\", \"solution\"], }, }\\n# Need gpt-4 to solve this one correctly prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Write out the following equation using algebraic symbols then solve it. \", ), (\"human\", \"{equation_statement}\"), ] ) model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind( function_call={\"name\": \"solver\"}, functions=[function] ) runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model runnable.invoke(\"x raised to the third plus seven equals 12\")\\nAIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'solver\\', \\'arguments\\': \\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = ∛5\"\\\\n}\\'}}, example=False)\\nAttaching OpenAI tools tools = [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\", }, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}, }, \"required\": [\"location\"], }, }, } ]\\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools) model.invoke(\"What\\'s the weather in SF, NYC and LA?\")', metadata={'source': './files/1_2_1_Bind_runtime_args.txt'}),\n",
       " Document(page_content='model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools) model.invoke(\"What\\'s the weather in SF, NYC and LA?\")\\nAIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_zHN0ZHwrxM7nZDdqTp6dkPko\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_aqdMm9HBSlFW9c9rqxTa7eQv\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_cx8E567zcLzYV2WSWVgO63f1\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}]})', metadata={'source': './files/1_2_1_Bind_runtime_args.txt'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\n",
    "    file_path=\"./files/1_2_1_Bind_runtime_args.txt\",\n",
    "    encoding='UTF-8'\n",
    ")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Introduction LangChain is a framework for developing applications powered by language models. It enables applications that:\\nAre context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) This framework consists of several parts.\\nLangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks. LangServe: A library for deploying LangChain chains as a REST API. LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain. LangChain Diagram\\nTogether, these products simplify the entire application lifecycle:\\nDevelop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. Deploy: Turn any chain into an API with LangServe. LangChain Libraries The main value props of the LangChain packages are:\\nComponents: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.\\nGet started Here’s how to install LangChain, set up your environment, and start building.\\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\\nRead up on our Security best practices to make sure you're developing safely with LangChain.\\nNOTE These docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nLangChain Expression Language (LCEL) LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.\\nOverview: LCEL and its benefits Interface: The standard interface for LCEL objects How-to: Key features of LCEL Cookbook: Example code for accomplishing common tasks Modules LangChain provides standard, extendable interfaces and integrations for the following modules:\\nModel I/O\\nInterface with language models\\nRetrieval\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_1_introduction.txt'}),\n",
       " Document(page_content=\"Overview: LCEL and its benefits Interface: The standard interface for LCEL objects How-to: Key features of LCEL Cookbook: Example code for accomplishing common tasks Modules LangChain provides standard, extendable interfaces and integrations for the following modules:\\nModel I/O\\nInterface with language models\\nRetrieval\\nInterface with application\\nspecific data\\nAgents Let models choose which tools to use given high-level directives\\nExamples, ecosystem, and resources Use cases Walkthroughs and techniques for common end-to-end use cases, like:\\nDocument question answering Chatbots Analyzing structured data and much more... Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\nGuides Best practices for developing with LangChain.\\nAPI reference Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\\nDeveloper's guide Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\nCommunity Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM’s.\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_1_introduction.txt'}),\n",
       " Document(page_content='Installation\\nOfficial release\\nTo install LangChain run:\\nPip\\nConda\\npip install langchain\\nThis will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately.\\nFrom source If you want to install from source, you can do so by cloning the repo and be sure that the directory is PATH/TO/REPO/langchain/libs/langchain running:\\npip install\\ne .\\nLangChain experimental The langchain-experimental package holds experimental LangChain code, intended for research and experimental uses. Install with:\\npip install langchain\\nexperimental\\nLangServe LangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with:\\npip install \"langserve[all]\"\\nfor both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\\nLangChain CLI The LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with:\\npip install langchain\\ncli\\nLangSmith SDK The LangSmith SDK is automatically installed by LangChain. If not using LangChain, install with:\\npip install langsmith', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_2_install.txt'}),\n",
       " Document(page_content='Quickstart In this quickstart we\\'ll show you how to:\\nGet setup with LangChain, LangSmith and LangServe Use the most basic and common components of LangChain: prompt templates, models, and output parsers Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining Build a simple application with LangChain Trace your application with LangSmith Serve your application with LangServe That\\'s a fair amount to cover! Let\\'s dive in.\\nSetup\\nInstallation\\nTo install LangChain run:\\nPip\\nConda\\npip install langchain\\nFor more details, see our Installation guide.\\nEnvironment Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we\\'ll use OpenAI\\'s model APIs.\\nFirst we\\'ll need to install their Python package:\\npip install openai\\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:\\nexport OPENAI_API_KEY=\"...\"\\nIf you\\'d prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\\nfrom langchain.chat_models import ChatOpenAI\\nllm = ChatOpenAI(openai_api_key=\"...\")\\nLangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGCHAIN_TRACING_V2=\"true\"\\nexport LANGCHAIN_API_KEY=...\\nLangServe LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\\'ll show how you can deploy your app with LangServe.\\nInstall with:\\npip install \"langserve[all]\"\\nBuilding with LangChain LangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by LangChain Expression Language (LCEL), which defines a unified Runnable interface that many modules implement, making it possible to seamlessly chain components.\\nThe simplest and most common chain contains three things:', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='pip install \"langserve[all]\"\\nBuilding with LangChain LangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by LangChain Expression Language (LCEL), which defines a unified Runnable interface that many modules implement, making it possible to seamlessly chain components.\\nThe simplest and most common chain contains three things:\\nLLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream. In this guide we\\'ll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler.\\nLLM / Chat Model There are two types of language models:\\nLLM: underlying model takes a string as input and returns a string ChatModel: underlying model takes a list of messages as input and returns a message Strings are simple, but what exactly are messages? The base message interface is defined by BaseMessage, which has two required attributes:\\ncontent: The content of the message. Usually a string. role: The entity from which the BaseMessage is coming. LangChain provides several objects to easily distinguish between different roles:\\nHumanMessage: A BaseMessage coming from a human/user. AIMessage: A BaseMessage coming from an AI/assistant. SystemMessage: A BaseMessage coming from the system. FunctionMessage / ToolMessage: A BaseMessage containing the output of a function or tool call. If none of those roles sound right, there is also a ChatMessage class where you can specify the role manually.\\nLangChain provides a common interface that\\'s shared by both LLMs and ChatModels. However it\\'s useful to understand the difference in order to most effectively construct prompts for a given language model.\\nThe simplest way to call an LLM or ChatModel is using .invoke(), the universal synchronous call method for all LangChain Expression Language (LCEL) objects:', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='LangChain provides a common interface that\\'s shared by both LLMs and ChatModels. However it\\'s useful to understand the difference in order to most effectively construct prompts for a given language model.\\nThe simplest way to call an LLM or ChatModel is using .invoke(), the universal synchronous call method for all LangChain Expression Language (LCEL) objects:\\nLLM.invoke: Takes in a string, returns a string. ChatModel.invoke: Takes in a list of BaseMessage, returns a BaseMessage. The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages. Check out the \"Go deeper\" section below to learn more about model invocation.\\nLet\\'s see how to work with these different types of models and these different types of inputs. First, let\\'s import an LLM and a ChatModel.\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nllm = OpenAI()\\nchat_model = ChatOpenAI()\\nLLM and ChatModel objects are effectively configuration objects. You can initialize them with parameters like temperature and others, and pass them around.\\nfrom langchain.schema import HumanMessage\\ntext = \"What would be a good company name for a company that makes colorful socks?\" messages = [HumanMessage(content=text)]\\nllm.invoke(text) # >> Feetful of Fun\\nchat_model.invoke(messages)\\n# >> AIMessage(content=\"Socks O\\'Color\")\\nGo deeper Prompt templates Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\\nPromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be:\\nfrom langchain.prompts import PromptTemplate\\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\") prompt.format(product=\"colorful socks\")\\nWhat is a good name for a company that makes colorful socks?', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='from langchain.prompts import PromptTemplate\\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\") prompt.format(product=\"colorful socks\")\\nWhat is a good name for a company that makes colorful socks?\\nHowever, the advantages of using these over raw string formatting are several. You can \"partial\" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the section on prompts for more detail.\\nPromptTemplates can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let\\'s take a look at this below:\\nfrom langchain.prompts.chat import ChatPromptTemplate\\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\" human_template = \"{text}\"\\nchat_prompt = ChatPromptTemplate.from_messages([\\n(\"system\", template),\\n(\"human\", human_template),\\n])\\nchat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\n[ SystemMessage(content=\"You are a helpful assistant that translates English to French. \", additional_kwargs={}), HumanMessage(content=\"I love programming.\") ]\\nChatPromptTemplates can also be constructed in other ways - see the section on prompts for more detail.\\nOutput parsers OutputParsers convert the raw output of a language model into a format that can be used downstream. There are few main types of OutputParsers, including:\\nConvert text from LLM into structured information (e.g. JSON) Convert a ChatMessage into just a string Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string. For full information on this, see the section on output parsers.\\nIn this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.\\nfrom langchain.schema import BaseOutputParser\\nclass CommaSeparatedListOutputParser(BaseOutputParser): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\\ndef parse(self, text: str): \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='from langchain.schema import BaseOutputParser\\nclass CommaSeparatedListOutputParser(BaseOutputParser): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\\ndef parse(self, text: str): \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \")\\nCommaSeparatedListOutputParser().parse(\"hi, bye\")\\n# >> [\\'hi\\', \\'bye\\']\\nComposing with LCEL We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let\\'s see it in action!\\nfrom typing import List\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import BaseOutputParser\\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\\ndef parse(self, text: str) -> List[str]: \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \")\\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\"\\nchat_prompt = ChatPromptTemplate.from_messages([ (\"system\", template), (\"human\", human_template), ]) chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() chain.invoke({\"text\": \"colors\"}) # >> [\\'red\\', \\'blue\\', \\'green\\', \\'yellow\\', \\'orange\\']\\nNote that we are using the | syntax to join these components together. This | syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal Runnable interface that all of these objects implement. To learn more about LCEL, read the documentation here.\\nTracing with LangSmith Assuming we\\'ve set our environment variables as shown in the beginning, all of the model and chain calls we\\'ve been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='Tracing with LangSmith Assuming we\\'ve set our environment variables as shown in the beginning, all of the model and chain calls we\\'ve been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application.\\nCheck out what the trace for the above chain would look like: https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r\\nFor more on LangSmith head here.\\nServing with LangServe Now that we\\'ve built an application, we need to serve it. That\\'s where LangServe comes in. LangServe helps developers deploy LCEL chains as a REST API. The library is integrated with FastAPI and uses pydantic for data validation.\\nServer To create a server for our application we\\'ll make a serve.py file with three things:\\nThe definition of our chain (same as above) Our FastAPI app A definition of a route from which to serve the chain, which is done with langserve.add_routes #!/usr/bin/env python from typing import List\\nfrom fastapi import FastAPI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema import BaseOutputParser\\nfrom langserve import add_routes\\n# 1. Chain definition\\nclass CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\\ndef parse(self, text: str) -> List[str]: \"\"\"Parse the output of an LLM call.\"\"\" return text.strip().split(\", \")\\ntemplate = \"\"\"You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more.\"\"\" human_template = \"{text}\"\\nchat_prompt = ChatPromptTemplate.from_messages([ (\"system\", template), (\"human\", human_template), ]) category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\\n# 2. App definition app = FastAPI( title=\"LangChain Server\", version=\"1.0\", description=\"A simple API server using LangChain\\'s Runnable interfaces\", )\\n# 3. Adding chain route add_routes( app, category_chain, path=\"/category_chain\", )\\nif __name__ == \"__main__\":\\nimport uvicorn', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content='# 2. App definition app = FastAPI( title=\"LangChain Server\", version=\"1.0\", description=\"A simple API server using LangChain\\'s Runnable interfaces\", )\\n# 3. Adding chain route add_routes( app, category_chain, path=\"/category_chain\", )\\nif __name__ == \"__main__\":\\nimport uvicorn\\nuvicorn.run(app, host=\"localhost\", port=8000)\\nAnd that\\'s it! If we execute this file:\\npython serve.py\\nwe should see our chain being served at localhost:8000.\\nPlayground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/category_chain/playground/ to try it out!\\nClient Now let\\'s set up a client for programmatically interacting with our service. We can easily do this with the langserve.RemoteRunnable. Using this, we can interact with the served chain as if it were running client-side.\\nfrom langserve import RemoteRunnable\\nremote_chain = RemoteRunnable(\"http://localhost:8000/category_chain/\") remote_chain.invoke({\"text\": \"colors\"}) # >> [\\'red\\', \\'blue\\', \\'green\\', \\'yellow\\', \\'orange\\']\\nTo learn more about the many other features of LangServe head here.\\nNext steps We\\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey:\\nRead up on LangChain Expression Language (LCEL) to learn how to chain these components together Dive deeper into LLMs, prompts, and output parsers and learn the other key components Explore common end-to-end use cases and template applications Read up on LangSmith, the platform for debugging, testing, monitoring and more Learn more about serving your applications with LangServe', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_3_quickstart.txt'}),\n",
       " Document(page_content=\"Security LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.\\nBest Practices When building such applications developers should remember to follow good security practices:\\nLimit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application. Anticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data. Defense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use. Risks of not doing so include, but are not limited to:\\nData corruption or loss. Unauthorized access to confidential information. Compromised performance or availability of critical resources. Example scenarios with mitigation strategies:\\nA user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials. If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_4_security.txt'}),\n",
       " Document(page_content='Reporting a Vulnerability Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.\\nEnterprise solutions LangChain may offer enterprise solutions for customers who have additional security requirements. Please contact us at sales@langchain.dev.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\0_4_security.txt'}),\n",
       " Document(page_content='LangChain Expression Language (LCEL) LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\\nStreaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\\nAsync support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\\nOptimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\\nRetries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\nAccess intermediate results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.\\nInput and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\nSeamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nSeamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_0_LCEL.txt'}),\n",
       " Document(page_content='Get started LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\\nBasic example: prompt + model + output parser The most basic and common use case is chaining a prompt template and a model together. To see how this works, let\\'s create a chain that takes a topic and generates a joke:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\") model = ChatOpenAI() output_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\nchain.invoke({\"topic\": \"ice cream\"})\\n\"Why did the ice cream go to therapy?\\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\"\\nNotice this line of this code, where we piece together then different components into a single chain using LCEL:\\nchain = prompt | model | output_parser\\nThe | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.\\nIn this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let\\'s take a look at each component individually to really understand what\\'s going on.\\n1. Prompt prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.\\nprompt_value = prompt.invoke({\"topic\": \"ice cream\"}) prompt_value\\nChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])\\nprompt_value.to_messages()\\n[HumanMessage(content=\\'tell me a short joke about ice cream\\')]\\nprompt_value.to_string()\\n\\'Human: tell me a short joke about ice cream\\'\\n2. Model The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.\\nmessage = model.invoke(prompt_value)\\nmessage', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_start.txt'}),\n",
       " Document(page_content='prompt_value.to_messages()\\n[HumanMessage(content=\\'tell me a short joke about ice cream\\')]\\nprompt_value.to_string()\\n\\'Human: tell me a short joke about ice cream\\'\\n2. Model The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.\\nmessage = model.invoke(prompt_value)\\nmessage\\nAIMessage(content=\"Why did the ice cream go to therapy? \\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\")\\nIf our model was an LLM, it would output a string.\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(model=\"gpt\\n3.5\\nturbo\\ninstruct\")\\nllm.invoke(prompt_value)\\n\\'\\\\n\\\\nRobot: Why did the ice cream go to therapy? Because it had a rocky road.\\'\\n3. Output parser And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string.\\noutput_parser.invoke(message)\\n\"Why did the ice cream go to therapy? \\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\"\\n4. Entire Pipeline To follow the steps along:\\nWe pass in user input on the desired topic as {\"topic\": \"ice cream\"} The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. Dict PromptValue ChatMessage String Input: topic=ice cream PromptTemplate ChatModel StrOutputParser Result INFO Note that if you’re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:\\ninput = {\"topic\": \"ice cream\"}\\nprompt.invoke(input) # > ChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])\\n(prompt | model).invoke(input) # > AIMessage(content=\"Why did the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn\\'t cone-trol itself!\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_start.txt'}),\n",
       " Document(page_content='input = {\"topic\": \"ice cream\"}\\nprompt.invoke(input) # > ChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])\\n(prompt | model).invoke(input) # > AIMessage(content=\"Why did the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn\\'t cone-trol itself!\")\\nRAG Search Example For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.\\n# Requires: # pip install langchain docarray\\nfrom langchain.chat_models import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableParallel, RunnablePassthrough from langchain.vectorstores import DocArrayInMemorySearch\\nvectorstore = DocArrayInMemorySearch.from_texts( [\"harrison worked at kensho\", \"bears like to eat honey\"], embedding=OpenAIEmbeddings(), ) retriever = vectorstore.as_retriever()\\ntemplate = \"\"\"Answer the question based only on the following context: {context}\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\noutput_parser = StrOutputParser()\\nsetup_and_retrieval = RunnableParallel( {\"context\": retriever, \"question\": RunnablePassthrough()} ) chain = setup_and_retrieval | prompt | model | output_parser\\nchain.invoke(\"where did harrison work?\")\\nIn this case, the composed chain is:\\nchain = setup_and_retrieval | prompt | model | output_parser\\nTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\\nAs a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\\nretriever.invoke(\"where did harrison work?\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_start.txt'}),\n",
       " Document(page_content='As a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\\nretriever.invoke(\"where did harrison work?\")\\nWe then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user’s question:\\nsetup_and_retrieval = RunnableParallel(\\n{\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\nTo review, the complete chain is:\\nsetup_and_retrieval = RunnableParallel( {\"context\": retriever, \"question\": RunnablePassthrough()} ) chain = setup_and_retrieval | prompt | model | output_parser\\nWith the flow being:\\nThe first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user’s original question. To pass on the question, we use RunnablePassthrough to copy this entry. Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. Question Question context=retrieved docs question=Question PromptValue ChatMessage String Question RunnableParallel Retriever RunnablePassThrough PromptTemplate ChatModel StrOutputParser Result Next steps We recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_start.txt'}),\n",
       " Document(page_content='Why use LCEL WE RECOMMEND READING THE LCEL GET STARTED SECTION FIRST. LCEL makes it easy to build complex chains from basic components. It does this by providing:\\nA unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more. To better understand the value of LCEL, it\\'s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we\\'ll do just that with our basic example from the get started section. We\\'ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nprompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\") model = ChatOpenAI(model=\"gpt-3.5-turbo\") output_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\nInvoke In the simplest case, we just want to pass in a topic string and get back a joke string:\\nWithout LCEL\\nfrom typing import List\\nimport openai\\nprompt_template = \"Tell me a short joke about {topic}\" client = openai.OpenAI()\\ndef call_chat_model(messages: List[dict])\\n> str:\\nresponse = client.chat.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\",\\nmessages=messages,\\n)\\nreturn response.choices[0].message.content\\ndef invoke_chain(topic: str) -> str: prompt_value = prompt_template.format(topic=topic) messages = [{\"role\": \"user\", \"content\": prompt_value}] return call_chat_model(messages)\\ninvoke_chain(\"ice cream\")\\nLCEL\\nfrom langchain_core.runnables import RunnablePassthrough', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content=')\\nreturn response.choices[0].message.content\\ndef invoke_chain(topic: str) -> str: prompt_value = prompt_template.format(topic=topic) messages = [{\"role\": \"user\", \"content\": prompt_value}] return call_chat_model(messages)\\ninvoke_chain(\"ice cream\")\\nLCEL\\nfrom langchain_core.runnables import RunnablePassthrough\\nprompt = ChatPromptTemplate.from_template( \"Tell me a short joke about {topic}\" ) output_parser = StrOutputParser() model = ChatOpenAI(model=\"gpt-3.5-turbo\") chain = ( {\"topic\": RunnablePassthrough()} | prompt | model | output_parser )\\nchain.invoke(\"ice cream\")\\nStream If we want to stream results instead, we\\'ll need to change our function:\\nWithout LCEL\\nfrom typing import Iterator\\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]: stream = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=messages, stream=True, ) for response in stream: content = response.choices[0].delta.content if content is not None: yield content\\ndef stream_chain(topic: str) -> Iterator[str]: prompt_value = prompt.format(topic=topic) return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])\\nfor chunk in stream_chain(\"ice cream\"): print(chunk, end=\"\", flush=True)\\nLCEL for chunk in chain.stream(\"ice cream\"): print(chunk, end=\"\", flush=True)\\nBatch If we want to run on a batch of inputs in parallel, we\\'ll again need a new function:\\nWithout LCEL\\nfrom concurrent.futures import ThreadPoolExecutor\\ndef batch_chain(topics: list)\\n> list:\\nwith ThreadPoolExecutor(max_workers=5) as executor:\\nreturn list(executor.map(invoke_chain, topics))\\nbatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])\\nLCEL\\nchain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\\nAsync If we need an asynchronous version:\\nWithout LCEL\\nasync_client = openai.AsyncOpenAI()\\nasync def acall_chat_model(messages: List[dict])\\n> str:\\nresponse = await async_client.chat.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\",\\nmessages=messages,\\n)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='Async If we need an asynchronous version:\\nWithout LCEL\\nasync_client = openai.AsyncOpenAI()\\nasync def acall_chat_model(messages: List[dict])\\n> str:\\nresponse = await async_client.chat.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\",\\nmessages=messages,\\n)\\nreturn response.choices[0].message.content\\nasync def ainvoke_chain(topic: str) -> str: prompt_value = prompt_template.format(topic=topic) messages = [{\"role\": \"user\", \"content\": prompt_value}] return await acall_chat_model(messages)\\nawait ainvoke_chain(\"ice cream\")\\nLCEL\\nchain.ainvoke(\"ice cream\")\\nLLM instead of chat model If we want to use a completion endpoint instead of a chat endpoint:\\nWithout LCEL\\ndef call_llm(prompt_value: str)\\n> str:\\nresponse = client.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\\ninstruct\",\\nprompt=prompt_value,\\n)\\nreturn response.choices[0].text\\ndef invoke_llm_chain(topic: str)\\n> str:\\nprompt_value = prompt_template.format(topic=topic)\\nreturn call_llm(prompt_value)\\ninvoke_llm_chain(\"ice cream\")\\nLCEL\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(model=\"gpt\\n3.5\\nturbo\\ninstruct\")\\nllm_chain = (\\n{\"topic\": RunnablePassthrough()}\\n| prompt\\n| llm\\n| output_parser\\n)\\nllm_chain.invoke(\"ice cream\")\\nDifferent model provider If we want to use Anthropic instead of OpenAI:\\nWithout LCEL\\nimport anthropic\\nanthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"\\nanthropic_client = anthropic.Anthropic()\\ndef call_anthropic(prompt_value: str)\\n> str:\\nresponse = anthropic_client.completions.create(\\nmodel=\"claude\\n2\",\\nprompt=prompt_value,\\nmax_tokens_to_sample=256,\\n)\\nreturn response.completion\\ndef invoke_anthropic_chain(topic: str)\\n> str:\\nprompt_value = anthropic_template.format(topic=topic)\\nreturn call_anthropic(prompt_value)\\ninvoke_anthropic_chain(\"ice cream\")\\nLCEL', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='model=\"claude\\n2\",\\nprompt=prompt_value,\\nmax_tokens_to_sample=256,\\n)\\nreturn response.completion\\ndef invoke_anthropic_chain(topic: str)\\n> str:\\nprompt_value = anthropic_template.format(topic=topic)\\nreturn call_anthropic(prompt_value)\\ninvoke_anthropic_chain(\"ice cream\")\\nLCEL\\nfrom langchain.chat_models import ChatAnthropic\\nanthropic = ChatAnthropic(model=\"claude\\n2\")\\nanthropic_chain = (\\n{\"topic\": RunnablePassthrough()}\\n| prompt\\n| anthropic\\n| output_parser\\n)\\nanthropic_chain.invoke(\"ice cream\")\\nRuntime configurability If we wanted to make the choice of chat model or LLM configurable at runtime:\\nWithout LCEL def invoke_configurable_chain( topic: str, *, model: str = \"chat_openai\" ) -> str: if model == \"chat_openai\": return invoke_chain(topic) elif model == \"openai\": return invoke_llm_chain(topic) elif model == \"anthropic\": return invoke_anthropic_chain(topic) else: raise ValueError( f\"Received invalid model \\'{model}\\'.\" \" Expected one of chat_openai, openai, anthropic\" )\\ndef stream_configurable_chain( topic: str, *, model: str = \"chat_openai\" ) -> Iterator[str]: if model == \"chat_openai\": return stream_chain(topic) elif model == \"openai\": # Note we haven\\'t implemented this yet. return stream_llm_chain(topic) elif model == \"anthropic\": # Note we haven\\'t implemented this yet return stream_anthropic_chain(topic) else: raise ValueError( f\"Received invalid model \\'{model}\\'.\" \" Expected one of chat_openai, openai, anthropic\" )\\ndef batch_configurable_chain( topics: List[str], *, model: str = \"chat_openai\" ) -> List[str]: # You get the idea ...\\nasync def abatch_configurable_chain(\\ntopics: List[str],\\n,\\nmodel: str = \"chat_openai\"\\n)\\n> List[str]:\\n...\\ninvoke_configurable_chain(\"ice cream\", model=\"openai\")\\nstream = stream_configurable_chain(\\n\"ice_cream\",\\nmodel=\"anthropic\"\\n)\\nfor chunk in stream:\\nprint(chunk, end=\"\", flush=True)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='topics: List[str],\\n,\\nmodel: str = \"chat_openai\"\\n)\\n> List[str]:\\n...\\ninvoke_configurable_chain(\"ice cream\", model=\"openai\")\\nstream = stream_configurable_chain(\\n\"ice_cream\",\\nmodel=\"anthropic\"\\n)\\nfor chunk in stream:\\nprint(chunk, end=\"\", flush=True)\\n# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"]) # await ainvoke_configurable_chain(\"ice cream\")\\nWith LCEL\\nfrom langchain_core.runnables import ConfigurableField\\nconfigurable_model = model.configurable_alternatives(\\nConfigurableField(id=\"model\"),\\ndefault_key=\"chat_openai\",\\nopenai=llm,\\nanthropic=anthropic,\\n)\\nconfigurable_chain = (\\n{\"topic\": RunnablePassthrough()}\\n| prompt\\n| configurable_model\\n| output_parser\\n)\\nconfigurable_chain.invoke(\\n\"ice cream\",\\nconfig={\"model\": \"openai\"}\\n)\\nstream = configurable_chain.stream(\\n\"ice cream\",\\nconfig={\"model\": \"anthropic\"}\\n)\\nfor chunk in stream:\\nprint(chunk, end=\"\", flush=True)\\nconfigurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\\n# await configurable_chain.ainvoke(\"ice cream\")\\nLogging If we want to log our intermediate results:\\nWithout LCEL We\\'ll print intermediate steps for illustrative purposes\\ndef invoke_anthropic_chain_with_logging(topic: str)\\n> str:\\nprint(f\"Input: {topic}\")\\nprompt_value = anthropic_template.format(topic=topic)\\nprint(f\"Formatted prompt: {prompt_value}\")\\noutput = call_anthropic(prompt_value)\\nprint(f\"Output: {output}\")\\nreturn output\\ninvoke_anthropic_chain_with_logging(\"ice cream\")\\nLCEL Every component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.\\nimport os\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nanthropic_chain.invoke(\"ice cream\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='LCEL Every component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.\\nimport os\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nanthropic_chain.invoke(\"ice cream\")\\nHere\\'s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r\\nFallbacks If we wanted to add fallback logic, in case one model API is down:\\nWithout LCEL\\ndef invoke_chain_with_fallback(topic: str)\\n> str:\\ntry:\\nreturn invoke_chain(topic)\\nexcept Exception:\\nreturn invoke_anthropic_chain(topic)\\nasync def ainvoke_chain_with_fallback(topic: str) -> str: try: return await ainvoke_chain(topic) except Exception: # Note: we haven\\'t actually implemented this. return ainvoke_anthropic_chain(topic)\\nasync def batch_chain_with_fallback(topics: List[str]) -> str: try: return batch_chain(topics) except Exception: # Note: we haven\\'t actually implemented this. return batch_anthropic_chain(topics)\\ninvoke_chain_with_fallback(\"ice cream\")\\n# await ainvoke_chain_with_fallback(\"ice cream\")\\nbatch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))\\nLCEL\\nfallback_chain = chain.with_fallbacks([anthropic_chain])\\nfallback_chain.invoke(\"ice cream\")\\n# await fallback_chain.ainvoke(\"ice cream\")\\nfallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])\\nFull code comparison Even in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.\\nWithout LCEL from concurrent.futures import ThreadPoolExecutor from typing import Iterator, List, Tuple\\nimport anthropic\\nimport openai\\nprompt_template = \"Tell me a short joke about {topic}\" anthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\" client = openai.OpenAI() async_client = openai.AsyncOpenAI() anthropic_client = anthropic.Anthropic()\\ndef call_chat_model(messages: List[dict])\\n> str:', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='import anthropic\\nimport openai\\nprompt_template = \"Tell me a short joke about {topic}\" anthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\" client = openai.OpenAI() async_client = openai.AsyncOpenAI() anthropic_client = anthropic.Anthropic()\\ndef call_chat_model(messages: List[dict])\\n> str:\\nresponse = client.chat.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\",\\nmessages=messages,\\n)\\nreturn response.choices[0].message.content\\ndef invoke_chain(topic: str) -> str: print(f\"Input: {topic}\") prompt_value = prompt_template.format(topic=topic) print(f\"Formatted prompt: {prompt_value}\") messages = [{\"role\": \"user\", \"content\": prompt_value}] output = call_chat_model(messages) print(f\"Output: {output}\") return output\\ndef stream_chat_model(messages: List[dict]) -> Iterator[str]: stream = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=messages, stream=True, ) for response in stream: content = response.choices[0].delta.content if content is not None: yield content\\ndef stream_chain(topic: str) -> Iterator[str]: print(f\"Input: {topic}\") prompt_value = prompt.format(topic=topic) print(f\"Formatted prompt: {prompt_value}\") stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}]) for chunk in stream: print(f\"Token: {chunk}\", end=\"\") yield chunk\\ndef batch_chain(topics: list)\\n> list:\\nwith ThreadPoolExecutor(max_workers=5) as executor:\\nreturn list(executor.map(invoke_chain, topics))\\ndef call_llm(prompt_value: str)\\n> str:\\nresponse = client.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\\ninstruct\",\\nprompt=prompt_value,\\n)\\nreturn response.choices[0].text\\ndef invoke_llm_chain(topic: str)\\n> str:\\nprint(f\"Input: {topic}\")\\nprompt_value = promtp_template.format(topic=topic)\\nprint(f\"Formatted prompt: {prompt_value}\")\\noutput = call_llm(prompt_value)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='instruct\",\\nprompt=prompt_value,\\n)\\nreturn response.choices[0].text\\ndef invoke_llm_chain(topic: str)\\n> str:\\nprint(f\"Input: {topic}\")\\nprompt_value = promtp_template.format(topic=topic)\\nprint(f\"Formatted prompt: {prompt_value}\")\\noutput = call_llm(prompt_value)\\nprint(f\"Output: {output}\")\\nreturn output\\ndef call_anthropic(prompt_value: str)\\n> str:\\nresponse = anthropic_client.completions.create(\\nmodel=\"claude\\n2\",\\nprompt=prompt_value,\\nmax_tokens_to_sample=256,\\n)\\nreturn response.completion\\ndef invoke_anthropic_chain(topic: str)\\n> str:\\nprint(f\"Input: {topic}\")\\nprompt_value = anthropic_template.format(topic=topic)\\nprint(f\"Formatted prompt: {prompt_value}\")\\noutput = call_anthropic(prompt_value)\\nprint(f\"Output: {output}\")\\nreturn output\\nasync def ainvoke_anthropic_chain(topic: str)\\n> str:\\n...\\ndef stream_anthropic_chain(topic: str)\\n> Iterator[str]:\\n...\\ndef batch_anthropic_chain(topics: List[str])\\n> List[str]:\\n...\\ndef invoke_configurable_chain( topic: str, *, model: str = \"chat_openai\" ) -> str: if model == \"chat_openai\": return invoke_chain(topic) elif model == \"openai\": return invoke_llm_chain(topic) elif model == \"anthropic\": return invoke_anthropic_chain(topic) else: raise ValueError( f\"Received invalid model \\'{model}\\'.\" \" Expected one of chat_openai, openai, anthropic\" )\\ndef stream_configurable_chain( topic: str, *, model: str = \"chat_openai\" ) -> Iterator[str]: if model == \"chat_openai\": return stream_chain(topic) elif model == \"openai\": # Note we haven\\'t implemented this yet. return stream_llm_chain(topic) elif model == \"anthropic\": # Note we haven\\'t implemented this yet return stream_anthropic_chain(topic) else: raise ValueError( f\"Received invalid model \\'{model}\\'.\" \" Expected one of chat_openai, openai, anthropic\" )\\ndef batch_configurable_chain(', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='def batch_configurable_chain(\\ntopics: List[str],\\n,\\nmodel: str = \"chat_openai\"\\n)\\n> List[str]:\\n...\\nasync def abatch_configurable_chain(\\ntopics: List[str],\\n,\\nmodel: str = \"chat_openai\"\\n)\\n> List[str]:\\n...\\ndef invoke_chain_with_fallback(topic: str)\\n> str:\\ntry:\\nreturn invoke_chain(topic)\\nexcept Exception:\\nreturn invoke_anthropic_chain(topic)\\nasync def ainvoke_chain_with_fallback(topic: str)\\n> str:\\ntry:\\nreturn await ainvoke_chain(topic)\\nexcept Exception:\\nreturn ainvoke_anthropic_chain(topic)\\nasync def batch_chain_with_fallback(topics: List[str])\\n> str:\\ntry:\\nreturn batch_chain(topics)\\nexcept Exception:\\nreturn batch_anthropic_chain(topics)\\nLCEL\\nimport os\\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI from langchain.llms import OpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nprompt = ChatPromptTemplate.from_template( \"Tell me a short joke about {topic}\" ) chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\") openai = OpenAI(model=\"gpt-3.5-turbo-instruct\") anthropic = ChatAnthropic(model=\"claude-2\") model = ( chat_openai .with_fallbacks([anthropic]) .configurable_alternatives( ConfigurableField(id=\"model\"), default_key=\"chat_openai\", openai=openai, anthropic=anthropic, ) )\\nchain = (\\n{\"topic\": RunnablePassthrough()}\\n| prompt\\n| model\\n| StrOutputParser()\\n)\\nNext steps To continue learning about LCEL, we recommend:', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='chain = (\\n{\"topic\": RunnablePassthrough()}\\n| prompt\\n| model\\n| StrOutputParser()\\n)\\nNext steps To continue learning about LCEL, we recommend:\\nReading up on the full LCEL Interface, which we\\'ve only partially covered here. Exploring the How-to section to learn about additional composition primitives that LCEL provides. Looking through the Cookbook section to see LCEL in action for common use cases. A good next use case to look at would be Retrieval-augmented generation.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'}),\n",
       " Document(page_content='Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.\\nSuppose we have a simple prompt + model sequence:\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nprompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Write out the following equation using algebraic symbols then solve it. Use the format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\", ), (\"human\", \"{equation_statement}\"), ] ) model = ChatOpenAI(temperature=0) runnable = ( {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser() )\\nprint(runnable.invoke(\"x raised to the third plus seven equals 12\"))\\nEQUATION: x^3 + 7 = 12\\nSOLUTION: Subtracting 7 from both sides of the equation, we get: x^3 = 12 - 7 x^3 = 5\\nTaking the cube root of both sides, we get: x = ∛5\\nTherefore, the solution to the equation x^3 + 7 = 12 is x = ∛5.\\nand want to call the model with certain stop words:\\nrunnable = ( {\"equation_statement\": RunnablePassthrough()} | prompt | model.bind(stop=\"SOLUTION\") | StrOutputParser() ) print(runnable.invoke(\"x raised to the third plus seven equals 12\"))\\nEQUATION: x^3 + 7 = 12\\nAttaching OpenAI functions One particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:\\nfunction = { \"name\": \"solver\", \"description\": \"Formulates and solves an equation\", \"parameters\": { \"type\": \"object\", \"properties\": { \"equation\": { \"type\": \"string\", \"description\": \"The algebraic expression of the equation\", }, \"solution\": { \"type\": \"string\", \"description\": \"The solution to the equation\", }, }, \"required\": [\"equation\", \"solution\"], }, }', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_1_Bind_runtime_args.txt'}),\n",
       " Document(page_content='function = { \"name\": \"solver\", \"description\": \"Formulates and solves an equation\", \"parameters\": { \"type\": \"object\", \"properties\": { \"equation\": { \"type\": \"string\", \"description\": \"The algebraic expression of the equation\", }, \"solution\": { \"type\": \"string\", \"description\": \"The solution to the equation\", }, }, \"required\": [\"equation\", \"solution\"], }, }\\n# Need gpt-4 to solve this one correctly prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"Write out the following equation using algebraic symbols then solve it. \", ), (\"human\", \"{equation_statement}\"), ] ) model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind( function_call={\"name\": \"solver\"}, functions=[function] ) runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | model runnable.invoke(\"x raised to the third plus seven equals 12\")\\nAIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'solver\\', \\'arguments\\': \\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = ∛5\"\\\\n}\\'}}, example=False)\\nAttaching OpenAI tools tools = [ { \"type\": \"function\", \"function\": { \"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\", }, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}, }, \"required\": [\"location\"], }, }, } ]\\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools) model.invoke(\"What\\'s the weather in SF, NYC and LA?\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_1_Bind_runtime_args.txt'}),\n",
       " Document(page_content='model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools) model.invoke(\"What\\'s the weather in SF, NYC and LA?\")\\nAIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_zHN0ZHwrxM7nZDdqTp6dkPko\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_aqdMm9HBSlFW9c9rqxTa7eQv\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_cx8E567zcLzYV2WSWVgO63f1\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}]})', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_1_Bind_runtime_args.txt'}),\n",
       " Document(page_content='Configure chain internals at runtime Oftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things. In order to make this experience as easy as possible, we have defined two methods.\\nFirst, a configurable_fields method. This lets you configure particular fields of a runnable.\\nSecond, a configurable_alternatives method. With this method, you can list out alternatives for any particular runnable that can be set during runtime.\\nConfiguration Fields With LLMs With LLMs we can configure things like temperature\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nmodel = ChatOpenAI(temperature=0).configurable_fields( temperature=ConfigurableField( id=\"llm_temperature\", name=\"LLM Temperature\", description=\"The temperature of the LLM\", ) )\\nmodel.invoke(\"pick a random number\")\\nAIMessage(content=\\'7\\')\\nmodel.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")\\nAIMessage(content=\\'34\\')\\nWe can also do this when its used as part of a chain\\nprompt = PromptTemplate.from_template(\"Pick a random number above {x}\") chain = prompt | model\\nchain.invoke({\"x\": 0})\\nAIMessage(content=\\'57\\')\\nchain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})\\nAIMessage(content=\\'6\\')\\nWith HubRunnables This is useful to allow for switching of prompts\\nfrom langchain.runnables.hub import HubRunnable\\nprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields( owner_repo_commit=ConfigurableField( id=\"hub_commit\", name=\"Hub Commit\", description=\"The Hub commit to pull from\", ) )\\nprompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})\\nChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer:\")])\\nprompt.with_config(configurable={\"hub_commit\": \"rlm/rag\\nprompt\\nllama\"}).invoke(\\n{\"question\": \"foo\", \"context\": \"bar\"}\\n)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_2_Configure_chain_internals_at_runtime.txt'}),\n",
       " Document(page_content='prompt.with_config(configurable={\"hub_commit\": \"rlm/rag\\nprompt\\nllama\"}).invoke(\\n{\"question\": \"foo\", \"context\": \"bar\"}\\n)\\nChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.<</SYS>> \\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer: [/INST]\")])\\nConfigurable Alternatives With LLMs Let\\'s take a look at doing this with LLMs\\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI from langchain.prompts import PromptTemplate from langchain.schema.runnable import ConfigurableField\\nllm = ChatAnthropic(temperature=0).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=\"llm\"), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=\"anthropic\", # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai=ChatOpenAI(), # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")` gpt4=ChatOpenAI(model=\"gpt-4\"), # You can add more configuration options here ) prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") chain = prompt | llm\\n# By default it will call Anthropic chain.invoke({\"topic\": \"bears\"})\\nAIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")\\n# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to use chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\\nAIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")\\n# If we use the `default_key` then it uses the default chain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_2_Configure_chain_internals_at_runtime.txt'}),\n",
       " Document(page_content='AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")\\n# If we use the `default_key` then it uses the default chain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})\\nAIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")\\nWith Prompts We can do a similar thing, but alternate between prompts\\nllm = ChatAnthropic(temperature=0) prompt = PromptTemplate.from_template( \"Tell me a joke about {topic}\" ).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=\"prompt\"), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=\"joke\", # This adds a new option, with name `poem` poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"), # You can add more configuration options here ) chain = prompt | llm\\n# By default it will write a joke chain.invoke({\"topic\": \"bears\"})\\nAIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")\\n# We can configure it write a poem chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})\\nAIMessage(content=\\' Here is a short poem about bears:\\\\n\\\\nThe bears awaken from their sleep\\\\nAnd lumber out into the deep\\\\nForests filled with trees so tall\\\\nForaging for food before nightfall \\\\nTheir furry coats and claws so sharp\\\\nSniffing for berries and fish to nab\\\\nLumbering about without a care\\\\nThe mighty grizzly and black bear\\\\nProud creatures, wild and free\\\\nRuling their domain majestically\\\\nWandering the woods they call their own\\\\nBefore returning to their dens alone\\')\\nWith Prompts and LLMs We can also have multiple things configurable! Here\\'s an example doing that with both prompts and LLMs.', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_2_Configure_chain_internals_at_runtime.txt'}),\n",
       " Document(page_content='With Prompts and LLMs We can also have multiple things configurable! Here\\'s an example doing that with both prompts and LLMs.\\nllm = ChatAnthropic(temperature=0).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=\"llm\"), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=\"anthropic\", # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai=ChatOpenAI(), # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")` gpt4=ChatOpenAI(model=\"gpt-4\"), # You can add more configuration options here ) prompt = PromptTemplate.from_template( \"Tell me a joke about {topic}\" ).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=\"prompt\"), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=\"joke\", # This adds a new option, with name `poem` poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"), # You can add more configuration options here ) chain = prompt | llm\\n# We can configure it write a poem with OpenAI chain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke( {\"topic\": \"bears\"} )', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_2_Configure_chain_internals_at_runtime.txt'}),\n",
       " Document(page_content='# We can configure it write a poem with OpenAI chain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke( {\"topic\": \"bears\"} )\\nAIMessage(content=\"In the forest, where tall trees sway,\\\\nA creature roams, both fierce and gray.\\\\nWith mighty paws and piercing eyes,\\\\nThe bear, a symbol of strength, defies.\\\\n\\\\nThrough snow-kissed mountains, it does roam,\\\\nA guardian of its woodland home.\\\\nWith fur so thick, a shield of might,\\\\nIt braves the coldest winter night.\\\\n\\\\nA gentle giant, yet wild and free,\\\\nThe bear commands respect, you see.\\\\nWith every step, it leaves a trace,\\\\nOf untamed power and ancient grace.\\\\n\\\\nFrom honeyed feast to salmon\\'s leap,\\\\nIt takes its place, in nature\\'s keep.\\\\nA symbol of untamed delight,\\\\nThe bear, a wonder, day and night.\\\\n\\\\nSo let us honor this noble beast,\\\\nIn forests where its soul finds peace.\\\\nFor in its presence, we come to know,\\\\nThe untamed spirit that in us also flows.\")\\n# We can always just configure only one if we want chain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})\\nAIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")\\nSaving configurations We can also easily save configured chains as their own objects\\nopenai_poem = chain.with_config(configurable={\"llm\": \"openai\"})\\nopenai_poem.invoke({\"topic\": \"bears\"})\\nAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_2_Configure_chain_internals_at_runtime.txt'}),\n",
       " Document(page_content='Add fallbacks There are many possible points of failure in an LLM application, whether that be issues with LLM API\\'s, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues.\\nCrucially, fallbacks can be applied not only on the LLM level but on the whole runnable level.\\nHandling LLM API Errors This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.\\nIMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.\\nfrom langchain.chat_models import ChatAnthropic, ChatOpenAI\\nFirst, let\\'s mock out what happens if we hit a RateLimitError from OpenAI\\nfrom unittest.mock import patch\\nimport httpx\\nfrom openai import RateLimitError\\nrequest = httpx.Request(\"GET\", \"/\") response = httpx.Response(200, request=request) error = RateLimitError(\"rate limit\", response=response, body=\"\")\\n# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm])\\n# Let\\'s use just the OpenAI LLm first, to show that we run into an error with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error): try: print(openai_llm.invoke(\"Why did the chicken cross the road?\")) except: print(\"Hit error\")\\nHit error\\n# Now let\\'s try with fallbacks to Anthropic with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error): try: print(llm.invoke(\"Why did the chicken cross the road?\")) except: print(\"Hit error\")', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_3_add_feedback.txt'}),\n",
       " Document(page_content='Hit error\\n# Now let\\'s try with fallbacks to Anthropic with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error): try: print(llm.invoke(\"Why did the chicken cross the road?\")) except: print(\"Hit error\")\\ncontent=\\' I don\\\\\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\\\n\\\\n- To get to the other side!\\\\n\\\\n- It was too chicken to just stand there. \\\\n\\\\n- It wanted a change of scenery.\\\\n\\\\n- It wanted to show the possum it could be done.\\\\n\\\\n- It was on its way to a poultry farmers\\\\\\' convention.\\\\n\\\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\\\\\'s an anti-joke, with a silly or unexpected pun as the answer.\\' additional_kwargs={} example=False\\nWe can use our \"LLM with Fallbacks\" as we would a normal LLM.\\nfrom langchain.prompts import ChatPromptTemplate\\nprompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You\\'re a nice assistant who always includes a compliment in your response\", ), (\"human\", \"Why did the {animal} cross the road\"), ] ) chain = prompt | llm with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error): try: print(chain.invoke({\"animal\": \"kangaroo\"})) except: print(\"Hit error\")\\ncontent=\" I don\\'t actually know why the kangaroo crossed the road, but I\\'m happy to take a guess! Maybe the kangaroo was trying to get to the other side to find some tasty grass to eat. Or maybe it was trying to get away from a predator or other danger. Kangaroos do need to cross roads and other open areas sometimes as part of their normal activities. Whatever the reason, I\\'m sure the kangaroo looked both ways before hopping across!\" additional_kwargs={} example=False\\nSpecifying errors to handle We can also specify the errors to handle if we want to be more specific about when the fallback is invoked:\\nllm = openai_llm.with_fallbacks(\\n[anthropic_llm], exceptions_to_handle=(KeyboardInterrupt,)\\n)\\nchain = prompt | llm with patch(\"openai.ChatCompletion.create\", side_effect=RateLimitError()): try: print(chain.invoke({\"animal\": \"kangaroo\"})) except: print(\"Hit error\")\\nHit error', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_3_add_feedback.txt'}),\n",
       " Document(page_content='llm = openai_llm.with_fallbacks(\\n[anthropic_llm], exceptions_to_handle=(KeyboardInterrupt,)\\n)\\nchain = prompt | llm with patch(\"openai.ChatCompletion.create\", side_effect=RateLimitError()): try: print(chain.invoke({\"animal\": \"kangaroo\"})) except: print(\"Hit error\")\\nHit error\\nFallbacks for Sequences We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.\\n# First let\\'s create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain.schema.output_parser import StrOutputParser\\nchat_prompt = ChatPromptTemplate.from_messages( [ ( \"system\", \"You\\'re a nice assistant who always includes a compliment in your response\", ), (\"human\", \"Why did the {animal} cross the road\"), ] ) # Here we\\'re going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=\"gpt-fake\") bad_chain = chat_prompt | chat_model | StrOutputParser()\\n# Now lets create a chain with the normal OpenAI model from langchain.llms import OpenAI from langchain.prompts import PromptTemplate\\nprompt_template = \"\"\"Instructions: You should always include a compliment in your response.\\nQuestion: Why did the {animal} cross the road?\"\"\" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm\\n# We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({\"animal\": \"turtle\"})\\n\\'\\\\n\\\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.\\'', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_3_add_feedback.txt'}),\n",
       " Document(page_content='Run custom functions You can use arbitrary functions in the pipeline\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument.\\nfrom operator import itemgetter\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.runnable import RunnableLambda\\ndef length_function(text):\\nreturn len(text)\\ndef _multiple_length_function(text1, text2):\\nreturn len(text1)\\nlen(text2)\\ndef multiple_length_function(_dict):\\nreturn _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\\nprompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\") model = ChatOpenAI()\\nchain1 = prompt | model\\nchain = ( { \"a\": itemgetter(\"foo\") | RunnableLambda(length_function), \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function), } | prompt | model )\\nchain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\\nAIMessage(content=\\'3 + 9 equals 12. \\', additional_kwargs={}, example=False)\\nAccepting a Runnable Config Runnable lambdas can optionally accept a RunnableConfig, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnableConfig\\nimport json\\ndef parse_or_fix(text: str, config: RunnableConfig): fixing_chain = ( ChatPromptTemplate.from_template( \"Fix the following text:\\\\n\\\\n```text\\\\n{input}\\\\n```\\\\nError: {error}\" \" Don\\'t narrate, just respond with the fixed data.\" ) | ChatOpenAI() | StrOutputParser() ) for _ in range(3): try: return json.loads(text) except Exception as e: text = fixing_chain.invoke({\"input\": text, \"error\": e}, config) return \"Failed to parse\"\\nfrom langchain.callbacks import get_openai_callback\\nwith get_openai_callback() as cb:\\nRunnableLambda(parse_or_fix).invoke(\\n\"{foo: bar}\", {\"tags\": [\"my', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_4_Run_custom_functions.txt'}),\n",
       " Document(page_content='from langchain.callbacks import get_openai_callback\\nwith get_openai_callback() as cb:\\nRunnableLambda(parse_or_fix).invoke(\\n\"{foo: bar}\", {\"tags\": [\"my\\ntag\"], \"callbacks\": [cb]}\\n)\\nprint(cb)\\nTokens Used: 65\\nPrompt Tokens: 56\\nCompletion Tokens: 9\\nSuccessful Requests: 1\\nTotal Cost (USD): $0.00010200000000000001', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_4_Run_custom_functions.txt'}),\n",
       " Document(page_content='Stream custom generator functions You can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser modifying the output of a previous step, while preserving streaming capabilities Let\\'s implement a custom output parser for comma-separated lists.\\nfrom typing import Iterator, List\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nprompt = ChatPromptTemplate.from_template( \"Write a comma-separated list of 5 animals similar to: {animal}\" ) model = ChatOpenAI(temperature=0.0)\\nstr_chain = prompt | model | StrOutputParser()\\nfor chunk in str_chain.stream({\"animal\": \"bear\"}): print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nstr_chain.invoke({\"animal\": \"bear\"})\\n\\'lion, tiger, wolf, gorilla, panda\\'\\n# This is a custom parser that splits an iterator of llm tokens # into a list of strings separated by commas def split_into_list(input: Iterator[str]) -> Iterator[List[str]]: # hold partial input until we get a comma buffer = \"\" for chunk in input: # add current chunk to buffer buffer += chunk # while there are commas in the buffer while \",\" in buffer: # split buffer on comma comma_index = buffer.index(\",\") # yield everything before the comma yield [buffer[:comma_index].strip()] # save the rest for the next iteration buffer = buffer[comma_index + 1 :] # yield the last chunk yield [buffer.strip()]\\nlist_chain = str_chain | split_into_list\\nfor chunk in list_chain.stream({\"animal\": \"bear\"}): print(chunk, flush=True)\\n[\\'lion\\']\\n[\\'tiger\\']\\n[\\'wolf\\']\\n[\\'gorilla\\']\\n[\\'panda\\']\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_5_Stream_custom_generator_functions.txt'}),\n",
       " Document(page_content='Interface To make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:\\nstream: stream back chunks of the response invoke: call the chain on an input batch: call the chain on a list of inputs These also have corresponding async methods:\\nastream: stream back chunks of the response async ainvoke: call the chain on an input async abatch: call the chain on a list of inputs async astream_log: stream back intermediate steps as they happen, in addition to the final response The input type and output type varies by component:\\nComponent\\tInput Type\\tOutput Type Prompt\\tDictionary\\tPromptValue ChatModel\\tSingle string, list of chat messages or a PromptValue\\tChatMessage LLM\\tSingle string, list of chat messages or a PromptValue\\tString OutputParser\\tThe output of an LLM or ChatModel\\tDepends on the parser Retriever\\tSingle string\\tList of Documents Tool\\tSingle string or dictionary, depending on the tool\\tDepends on the tool All runnables expose input and output schemas to inspect the inputs and outputs:\\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable output_schema: an output Pydantic model auto-generated from the structure of the Runnable Let\\'s take a look at these methods. To do so, we\\'ll create a super simple PromptTemplate + ChatModel chain.\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nmodel = ChatOpenAI() prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") chain = prompt | model\\nInput Schema A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\\n# The input schema of the chain is the input schema of its first part, the prompt. chain.input_schema.schema()\\n{\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}\\nprompt.input_schema.schema()\\n{\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}\\nmodel.input_schema.schema()', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"{'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\\nprompt.input_schema.schema()\\n{'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\\nmodel.input_schema.schema()\\n{'title': 'ChatOpenAIInput', 'anyOf': [{'type': 'string'}, {'$ref': '#/definitions/StringPromptValue'}, {'$ref': '#/definitions/ChatPromptValueConcrete'}, {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}]}}], 'definitions': {'StringPromptValue': {'title': 'StringPromptValue', 'description': 'String prompt value. ', 'type': 'object', 'properties': {'text': {'title': 'Text', 'type': 'string'}, 'type': {'title': 'Type', 'default': 'StringPromptValue', 'enum': ['StringPromptValue'], 'type': 'string'}}, 'required': ['text']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human.\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role). ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'chat', 'enum': ['chat'], 'type': 'string'}, 'role': {'title': 'Role', 'type': 'string'}}, 'required': ['content', 'role']}, 'SystemMessage': {'title': 'SystemMessage', 'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'system', 'enum': ['system'], 'type': 'string'}}, 'required': ['content']}, 'FunctionMessage': {'title': 'FunctionMessage', 'description': 'A Message for passing the result of executing a function back to a model.\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'function', 'enum': ['function'], 'type': 'string'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['content', 'name']}, 'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete', 'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\\\nFor use in external schemas. ', 'type': 'object', 'properties': {'messages': {'title': 'Messages', 'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/SystemMessage'}, {'$ref': '#/definitions/FunctionMessage'}]}}, 'type': {'title': 'Type', 'default': 'ChatPromptValueConcrete', 'enum': ['ChatPromptValueConcrete'], 'type': 'string'}}, 'required': ['messages']}}}\\nOutput Schema A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\\n# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain.output_schema.schema()\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"Output Schema A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.\\n# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain.output_schema.schema()\\n{'title': 'ChatOpenAIOutput', 'anyOf': [{'$ref': '#/definitions/HumanMessage'}, {'$ref': '#/definitions/AIMessage'}, {'$ref': '#/definitions/ChatMessage'}, {'$ref': '#/definitions/FunctionMessage'}, {'$ref': '#/definitions/SystemMessage'}], 'definitions': {'HumanMessage': {'title': 'HumanMessage', 'description': 'A Message from a human. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'human', 'enum': ['human'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'AIMessage': {'title': 'AIMessage', 'description': 'A Message from an AI. ', 'type': 'object', 'properties': {'content': {'title': 'Content', 'type': 'string'}, 'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'}, 'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'}, 'example': {'title': 'Example', 'default': False, 'type': 'boolean'}}, 'required': ['content']}, 'ChatMessage': {'title': 'ChatMessage', 'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='\\', \\'type\\': \\'object\\', \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\', \\'type\\': \\'string\\'}, \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'}, \\'type\\': {\\'title\\': \\'Type\\', \\'default\\': \\'chat\\', \\'enum\\': [\\'chat\\'], \\'type\\': \\'string\\'}, \\'role\\': {\\'title\\': \\'Role\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'content\\', \\'role\\']}, \\'FunctionMessage\\': {\\'title\\': \\'FunctionMessage\\', \\'description\\': \\'A Message for passing the result of executing a function back to a model. \\', \\'type\\': \\'object\\', \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\', \\'type\\': \\'string\\'}, \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'}, \\'type\\': {\\'title\\': \\'Type\\', \\'default\\': \\'function\\', \\'enum\\': [\\'function\\'], \\'type\\': \\'string\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'content\\', \\'name\\']}, \\'SystemMessage\\': {\\'title\\': \\'SystemMessage\\', \\'description\\': \\'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages. \\', \\'type\\': \\'object\\', \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\', \\'type\\': \\'string\\'}, \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'}, \\'type\\': {\\'title\\': \\'Type\\', \\'default\\': \\'system\\', \\'enum\\': [\\'system\\'], \\'type\\': \\'string\\'}}, \\'required\\': [\\'content\\']}}}\\nStream for s in chain.stream({\"topic\": \"bears\"}): print(s.content, end=\"\", flush=True)\\nWhy don\\'t bears wear shoes?\\nBecause they already have bear feet!\\nInvoke\\nchain.invoke({\"topic\": \"bears\"})\\nAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")\\nBatch\\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\\n[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet! \"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]\\nYou can set the number of concurrent requests by using the max_concurrency parameter\\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet! \"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]\\nYou can set the number of concurrent requests by using the max_concurrency parameter\\nchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})\\n[AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they have bear feet! \"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]\\nAsync Stream async for s in chain.astream({\"topic\": \"bears\"}): print(s.content, end=\"\", flush=True)\\nSure, here\\'s a bear\\nthemed joke for you:\\nWhy don\\'t bears wear shoes?\\nBecause they already have bear feet!\\nAsync Invoke\\nawait chain.ainvoke({\"topic\": \"bears\"})\\nAIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they have bear feet!\")\\nAsync Batch\\nawait chain.abatch([{\"topic\": \"bears\"}])\\n[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")]\\nAsync Stream Intermediate Steps All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence.\\nThis is useful to show progress to the user, to use intermediate results, or to debug your chain.\\nYou can stream all steps (default) or include/exclude steps by name, tags or metadata.\\nThis method yields JSONPatch ops that when applied in the same order as received build up the RunState.\\nclass LogEntry(TypedDict): id: str \"\"\"ID of the sub-run.\"\"\" name: str \"\"\"Name of the object being run.\"\"\" type: str \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\" tags: List[str] \"\"\"List of tags for the run.\"\"\" metadata: Dict[str, Any] \"\"\"Key-value pairs of metadata for the run.\"\"\" start_time: str \"\"\"ISO-8601 timestamp of when the run started.\"\"\"', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='streamed_output_str: List[str] \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\" final_output: Optional[Any] \"\"\"Final output of this run. Only available after the run has finished successfully.\"\"\" end_time: Optional[str] \"\"\"ISO-8601 timestamp of when the run ended. Only available after the run has finished.\"\"\"\\nclass RunState(TypedDict): id: str \"\"\"ID of the run.\"\"\" streamed_output: List[Any] \"\"\"List of output chunks streamed by Runnable.stream()\"\"\" final_output: Optional[Any] \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output. Only available after the run has finished successfully.\"\"\"\\nlogs: Dict[str, LogEntry] \"\"\"Map of run names to sub-runs. If filters were supplied, this list will contain only the runs that matched the filters.\"\"\"\\nStreaming JSONPatch chunks This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.vectorstores import FAISS\\ntemplate = \"\"\"Answer the question based only on the following context: {context}\\nQuestion: {question}\\n\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nvectorstore = FAISS.from_texts( [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever()\\nretrieval_chain = (\\n{\\n\"context\": retriever.with_config(run_name=\"Docs\"),\\n\"question\": RunnablePassthrough(),\\n}\\n| prompt\\n| model\\n| StrOutputParser()\\n)\\nasync for chunk in retrieval_chain.astream_log( \"where did harrison work? \", include_names=[\"Docs\"] ): print(\"-\" * 40) print(chunk)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='retrieval_chain = (\\n{\\n\"context\": retriever.with_config(run_name=\"Docs\"),\\n\"question\": RunnablePassthrough(),\\n}\\n| prompt\\n| model\\n| StrOutputParser()\\n)\\nasync for chunk in retrieval_chain.astream_log( \"where did harrison work? \", include_names=[\"Docs\"] ): print(\"-\" * 40) print(chunk)\\n---------------------------------------- RunLogPatch({\\'op\\': \\'replace\\', \\'path\\': \\'\\', \\'value\\': {\\'final_output\\': None, \\'id\\': \\'e2f2cc72-eb63-4d20-8326-237367482efb\\', \\'logs\\': {}, \\'streamed_output\\': []}}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/logs/Docs\\', \\'value\\': {\\'end_time\\': None, \\'final_output\\': None, \\'id\\': \\'8da492cc-4492-4e74-b8b0-9e60e8693390\\', \\'metadata\\': {}, \\'name\\': \\'Docs\\', \\'start_time\\': \\'2023-10-19T17:50:13.526\\', \\'streamed_output_str\\': [], \\'tags\\': [\\'map:key:context\\', \\'FAISS\\'], \\'type\\': \\'retriever\\'}}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/logs/Docs/final_output\\', \\'value\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]}}, {\\'op\\': \\'add\\', \\'path\\': \\'/logs/Docs/end_time\\', \\'value\\': \\'2023-10-19T17:50:13.713\\'}) ---------------------------------------- RunLogPatch({\\'op\\':', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='\\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'H\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'arrison\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' worked\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' at\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' Kens\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'ho\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'.\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'}) ---------------------------------------- RunLogPatch({\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': {\\'output\\': \\'Harrison worked at Kensho.\\'}})\\nStreaming the incremental RunState You can simply pass diff=False to get incremental values of RunState. You get more verbose output with more repetitive parts.\\nasync for chunk in retrieval_chain.astream_log( \"where did harrison work? \", include_names=[\"Docs\"], diff=False ): print(\"-\" * 70) print(chunk)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': None, 'final_output': None, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name':\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens']}) ---------------------------------------------------------------------- RunLog({'final_output':\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content=\"None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho']}) ---------------------------------------------------------------------- RunLog({'final_output': None, 'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6', 'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738', 'final_output': {'documents': [Document(page_content='harrison worked at kensho')]}, 'id': '88d51118-5756-4891-89c5-2f6a5e90cc26', 'metadata': {}, 'name': 'Docs', 'start_time': '2023-10-19T17:52:15.438', 'streamed_output_str': [], 'tags': ['map:key:context', 'FAISS'], 'type': 'retriever'}}, 'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.']})\", metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='---------------------------------------------------------------------- RunLog({\\'final_output\\': None, \\'id\\': \\'afe66178-d75f-4c2d-b348-b1d144239cd6\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2023-10-19T17:52:15.738\\', \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]}, \\'id\\': \\'88d51118-5756-4891-89c5-2f6a5e90cc26\\', \\'metadata\\': {}, \\'name\\': \\'Docs\\', \\'start_time\\': \\'2023-10-19T17:52:15.438\\', \\'streamed_output_str\\': [], \\'tags\\': [\\'map:key:context\\', \\'FAISS\\'], \\'type\\': \\'retriever\\'}}, \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\', \\'. \\', \\'\\']}) ---------------------------------------------------------------------- RunLog({\\'final_output\\': {\\'output\\': \\'Harrison worked at Kensho. \\'}, \\'id\\': \\'afe66178-d75f-4c2d-b348-b1d144239cd6\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2023-10-19T17:52:15.738\\', \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]}, \\'id\\': \\'88d51118-5756-4891-89c5-2f6a5e90cc26\\', \\'metadata\\': {}, \\'name\\': \\'Docs\\', \\'start_time\\': \\'2023-10-19T17:52:15.438\\', \\'streamed_output_str\\': [], \\'tags\\': [\\'map:key:context\\', \\'FAISS\\'], \\'type\\': \\'retriever\\'}}, \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\', \\'. \\', \\'\\']})\\nParallelism Let\\'s take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.\\nfrom langchain.schema.runnable import RunnableParallel\\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model chain2 = ( ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model ) combined = RunnableParallel(joke=chain1, poem=chain2)', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='from langchain.schema.runnable import RunnableParallel\\nchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model chain2 = ( ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\") | model ) combined = RunnableParallel(joke=chain1, poem=chain2)\\nchain1.invoke({\"topic\": \"bears\"})\\nCPU times: user 54.3 ms, sys: 0 ns, total: 54.3 ms Wall time: 2.29 s\\nAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")\\nchain2.invoke({\"topic\": \"bears\"})\\nCPU times: user 7.8 ms, sys: 0 ns, total: 7.8 ms Wall time: 1.43 s\\nAIMessage(content=\"In wild embrace,\\\\nNature\\'s strength roams with grace.\")\\ncombined.invoke({\"topic\": \"bears\"})\\nCPU times: user 167 ms, sys: 921 µs, total: 168 ms Wall time: 1.56 s\\n{\\'joke\\': AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet! \"), \\'poem\\': AIMessage(content=\"Fierce and wild, nature\\'s might,\\\\nBears roam the woods, shadows of the night.\")}\\nParallelism on batches Parallelism can be combined with other runnables. Let\\'s try to use parallelism with batches.\\nchain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\\nCPU times: user 159 ms, sys: 3.66 ms, total: 163 ms Wall time: 1.34 s\\n[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet! \"), AIMessage(content=\"Sure, here\\'s a cat joke for you:\\\\n\\\\nWhy don\\'t cats play poker in the wild?\\\\n\\\\nBecause there are too many cheetahs!\")]\\nchain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\\nCPU times: user 165 ms, sys: 0 ns, total: 165 ms Wall time: 1.73 s\\n[AIMessage(content=\"Silent giants roam,\\\\nNature\\'s strength, love\\'s emblem shown. \"), AIMessage(content=\\'Whiskers aglow, paws tiptoe,\\\\nGraceful hunters, hearts aglow.\\')]\\ncombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'}),\n",
       " Document(page_content='CPU times: user 165 ms, sys: 0 ns, total: 165 ms Wall time: 1.73 s\\n[AIMessage(content=\"Silent giants roam,\\\\nNature\\'s strength, love\\'s emblem shown. \"), AIMessage(content=\\'Whiskers aglow, paws tiptoe,\\\\nGraceful hunters, hearts aglow.\\')]\\ncombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\\nCPU times: user 507 ms, sys: 125 ms, total: 632 ms Wall time: 1.49 s\\n[{\\'joke\\': AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet! \"), \\'poem\\': AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s wild guardians of home. \")}, {\\'joke\\': AIMessage(content=\"Sure, here\\'s a cat joke for you:\\\\n\\\\nWhy did the cat sit on the computer?\\\\n\\\\nBecause it wanted to keep an eye on the mouse! \"), \\'poem\\': AIMessage(content=\\'Whiskers twitch, eyes gleam,\\\\nGraceful creatures, feline dream.\\')}]', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_2_interface.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='../',\n",
    "    glob=\"**/files/*.txt\"\n",
    ")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
