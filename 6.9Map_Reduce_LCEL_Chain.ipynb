{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The benefits of using LCEL (Language Chain Execution Library) include:\\n\\n1. Unified interface: LCEL objects implement the Runnable interface, allowing for common invocation methods such as invoke, batch, stream, ainvoke, and more.\\n\\n2. Composition primitives: LCEL provides primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.\\n\\n3. Simplified chain building: LCEL simplifies the process of building complex chains from basic components, enabling the creation of sophisticated workflows.\\n\\n4. Reusability: Chains of LCEL objects are themselves LCEL objects, making them reusable and modular.\\n\\n5. Increased functionality: LCEL enhances the functionality of existing components and allows for the creation of new functionality by combining and configuring different components.\\n\\n6. Streaming support: LCEL enables streaming tokens directly from a Language Model to a streaming output parser, providing incremental chunks of output in real-time.\\n\\n7. Async support: LCEL chains can be called using both synchronous and asynchronous APIs, facilitating seamless transition between prototyping and production environments with high performance and concurrent request handling.\\n\\n8. Optimized parallel execution: LCEL automatically executes steps in parallel when possible, reducing latency and improving performance in both synchronous and asynchronous interfaces.\\n\\n9. Retries and fallbacks: LCEL chains can be configured with retries and fallbacks, enhancing reliability at scale. Streaming support for retries and fallbacks is also being developed to minimize latency.\\n\\n10. Access to intermediate results: LCEL allows for accessing intermediate results during the execution of complex chains, enabling real-time updates for end-users or debugging purposes.\\n\\n11. Input and output schemas: LCEL automatically generates Pydantic and JSONSchema schemas based on the chain's structure, facilitating input and output validation.\\n\\n12. Seamless integration with LangSmith tracing: LCEL automatically logs all steps to LangSmith, providing maximum observability and debuggability as chains become more complex.\\n\\n13. Seamless LangServe deployment integration: LCEL chains can be easily deployed using LangServe, simplifying the deployment process.\\n\\nOverall, LCEL simplifies the process of building and managing complex chains, providing a unified interface, composition primitives, and various other benefits for efficient and flexible chain construction.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "from langchain.\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='../',\n",
    "    glob=\"**/files/*.txt\"\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "        \"context\":doc.page_content,\n",
    "        \"question\":question\n",
    "    }).content for doc in documents)\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\" : retriever, \"question\": RunnablePassthrough()\n",
    "    } | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = { \"context\" : map_chain , \"question\" : RunnablePassthrough() } | final_prompt | llm \n",
    "\n",
    "chain.invoke(\"What is benefits of using LCEL\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
