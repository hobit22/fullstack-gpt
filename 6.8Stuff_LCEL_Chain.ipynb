{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The benefits of using LCEL (LangChain Expression Language) include:\\n\\n1. Unified Interface: Every LCEL object implements the Runnable interface, which allows chains of LCEL objects to support common invocation methods. This makes it easy to build complex chains from basic components.\\n\\n2. Composition Primitives: LCEL provides primitives that simplify the composition of chains, allowing for parallelization, fallbacks, dynamic configuration, and more.\\n\\n3. Streaming Support: LCEL chains are designed to have the best possible time-to-first-token, allowing for streaming of output in incremental chunks.\\n\\n4. Async Support: LCEL chains can be called both synchronously and asynchronously, enabling the same code to be used for prototypes and production with great performance and the ability to handle concurrent requests.\\n\\n5. Optimized Parallel Execution: LCEL automatically executes steps in parallel when possible, reducing latency.\\n\\n6. Retries and Fallbacks: LCEL chains can be configured with retries and fallbacks, improving reliability at scale.\\n\\n7. Access to Intermediate Results: Complex chains can provide access to intermediate results, allowing for real-time updates or debugging.\\n\\n8. Input and Output Schemas: LCEL chains have Pydantic and JSONSchema schemas inferred from their structure, enabling input and output validation.\\n\\n9. LangSmith Tracing Integration: All steps in LCEL chains are automatically logged to LangSmith for observability and debuggability.\\n\\n10. LangServe Deployment Integration: LCEL chains can be easily deployed using LangServe.\\n\\nThese benefits make LCEL a powerful tool for building and deploying complex chains of components.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='../',\n",
    "    glob=\"**/files/*.txt\"\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is benefits of using LCEL\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
