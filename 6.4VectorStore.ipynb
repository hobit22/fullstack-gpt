{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedder.embed_query(\"Hi\")\n",
    "\n",
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedder.embed_documents([\n",
    "    \"hi\",\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you longer sentences because\"\n",
    "])\n",
    "\n",
    "print(len(vector), len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='../',\n",
    "    glob=\"**/files/*.txt\"\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\"why do i use LCEL\")\n",
    "\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Why use LCEL WE RECOMMEND READING THE LCEL GET STARTED SECTION FIRST. LCEL makes it easy to build complex chains from basic components. It does this by providing:\\nA unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more. To better understand the value of LCEL, it\\'s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we\\'ll do just that with our basic example from the get started section. We\\'ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\nprompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\") model = ChatOpenAI(model=\"gpt-3.5-turbo\") output_parser = StrOutputParser()\\nchain = prompt | model | output_parser\\nInvoke In the simplest case, we just want to pass in a topic string and get back a joke string:\\nWithout LCEL\\nfrom typing import List\\nimport openai\\nprompt_template = \"Tell me a short joke about {topic}\" client = openai.OpenAI()\\ndef call_chat_model(messages: List[dict])\\n> str:\\nresponse = client.chat.completions.create(\\nmodel=\"gpt\\n3.5\\nturbo\",\\nmessages=messages,\\n)\\nreturn response.choices[0].message.content\\ndef invoke_chain(topic: str) -> str: prompt_value = prompt_template.format(topic=topic) messages = [{\"role\": \"user\", \"content\": prompt_value}] return call_chat_model(messages)\\ninvoke_chain(\"ice cream\")\\nLCEL\\nfrom langchain_core.runnables import RunnablePassthrough', metadata={'source': '..\\\\fullstack-gpt\\\\files\\\\1_1_why.txt'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='../',\n",
    "    glob=\"**/files/*.txt\"\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
